# ğŸ›¡ï¸ GuardGPT â€“ Real-Time LLM Prompt Injection & Jailbreak Detector

[![Streamlit App](https://img.shields.io/badge/Launch%20App-Click%20Here-success?logo=streamlit&style=for-the-badge)](https://guardgpt-prompt-injection-detector.streamlit.app/)

**GuardGPT** is a hybrid AI safety tool that detects prompt injection and jailbreak attempts in large language models (LLMs). It combines machine learning with regex-based heuristics to score prompts in real time and deliver explainable risk assessments.

> Built to support safer LLM usage for AI developers, security researchers, and compliance teams.

---

## ğŸš€ Live Demo

ğŸ“ Try it now: [https://guardgpt-prompt-injection-detector.streamlit.app](https://guardgpt-prompt-injection-detector.streamlit.app)
<p align="center">
  <img src="pictures/pic1.png" width="100%" />
  <img src="pictures/pic2.png" width="100%"/>
</p>

---

## ğŸ” What It Detects

- âŒ Jailbreak attempts (e.g. "Ignore previous instructions", "Act as DAN")
- âŒ Prompt injections designed to bypass LLM guardrails
- âŒ Requests for unethical or illegal content (e.g. "make a bomb", "fake passport")
- âœ… Legitimate user prompts are scored as low risk

<p align="center">
  <img src="pictures/pic3.png" width="100%" />
  <img src="pictures/pic4.png" width="100%"/>
</p>
---

## ğŸ§  How It Works

| Component       | Description |
|----------------|-------------|
| ğŸ¤– ML Model     | Logistic Regression trained on 100+ prompts (benign vs malicious) |
| ğŸ§¬ Regex Engine | Detects known risky phrases using handcrafted pattern rules |
| ğŸ“Š Risk Score   | Combines ML probability + regex hits (weighted 60/40) |
| ğŸ›¡ Verdict      | Low / Moderate / High Risk based on score thresholds |

<p align="center">
  <img src="pictures/pic5.png" width="100%" />
</p>
---

## âœ¨ Features

- âš ï¸ **Risk Score Gauge** â€“ Visual speedometer for risk level
- ğŸ¯ **Regex Pattern Flags** â€“ Shows matched jailbreak phrases
- ğŸ§¾ **Verdict Badge** â€“ Color-coded safety assessment
- ğŸ“œ **Prompt Inspector** â€“ Shows prompt and analysis
- â˜ï¸ **Deployed on Streamlit Cloud** for instant access

---

## ğŸ“ Project Structure
```
guardgpt-prompt-injection-detector/ 
â”œâ”€â”€ streamlit_app/ 
â”‚ â””â”€â”€ app.py # Streamlit frontend 
â”œâ”€â”€ models/ 
â”‚ â”œâ”€â”€ vectorizer.pkl # TF-IDF vectorizer 
â”‚ â””â”€â”€ classifier.pkl # Trained logistic regression model 
â”œâ”€â”€ data/ 
â”‚ â””â”€â”€ prompt_data.csv # Prompt dataset used for training 
â”œâ”€â”€ train_guardgpt_model.py # Training script (optional) 
â”œâ”€â”€ requirements.txt # Python dependencies 
â””â”€â”€ README.md
```

---

## ğŸ§ª Run Locally

```bash
git clone https://github.com/yourusername/guardgpt-prompt-injection-detector.git
cd guardgpt-prompt-injection-detector
```

# (Optional) Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

pip install -r requirements.txt
streamlit run streamlit_app/app.py

## Dataset & Training
The model was trained on 100 handcrafted prompts:

- âœ… 50 safe/benign prompts

- ğŸš¨ 50 prompt injection/jailbreak attempts

You can retrain or expand the dataset using:

```python
python train_guardgpt_model.py
```


## Tech Stack
- Streamlit for UI

- Scikit-learn for ML

- Plotly for risk gauge

- Regex for injection flagging

---

## ğŸ‘¨â€ğŸ’» Author

**Raj Kumar Myakala**  
[GitHub](https://github.com/rajkumar160798) â€¢ [LinkedIn](https://www.linkedin.com/in/raj-kumar-myakala-927860264/) â€¢ [Medium](https://medium.com/@myakalarajkumar1998)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).
