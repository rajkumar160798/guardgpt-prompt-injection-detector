# guardgpt-prompt-injection-detector
An open-source hybrid AI system to detect jailbreak attempts and prompt injections in large language models (LLMs) using regex patterns, ML classifiers, and anomaly detection. GuardGPT scores prompt risk in real-time and provides interpretable flags to ensure safe and responsible LLM usage. Built with FastAPI and Streamlit.
